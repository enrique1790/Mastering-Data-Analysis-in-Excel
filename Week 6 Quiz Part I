### Mentor Hints for Week 6 Quiz Part I (13 questions)
Hi Class, feel free to leave your questions below and I will answer them.

Click to download Excel workbook

tab 7-3
Training set: 1 - 200
Test set: 201 - 400

Common mistakes
When you use test dataset, make sure you use the outcome for the test dataset too.
Sample Answer for Final Project
https://www.coursera.org/learn/analytics-excel/supplement/OHBGM

## Question 1
Your model should have at least two data points. For instance, 
[Credit Score from -3.5 - 3.5] = constant + a*X1 + b*X2 + c*X3

Note: Although the quiz tool will tell you the answer you entered is correct, this is an automated answer. If fact, there is no 
exact answer and working through the final project will tell you if you have a viable model.

** To build your model**

There are many ways to build your model, and here is one simple way using Excel:
1) Identify highly correlated variables with the Outcome(the column with 1 and 0) by using LINEST function in Excel
2) Run regression analysis with the best correlated variables (Data analysis/Regression). More about Regression in Excel

Comment:
1 r

## Question 2
<.5 is not valid
.5 to < .59 is low value
.6 - .75 is ok
.75 - .86 is very good
>.86 is probably too good to be true

Comment:
12 x
6 x
.7 r
((((Less than .5 is not correct - you need to make the highest value the lowest by
dividing by -1.
.5 has no predictive value.
.9 or higher is too good to be true!))))

## Question 3
If the test set AUC is much worse than the training set AUC, consider making your model simpler – probably using fewer input variables
– as it is probably over-fitting the training set data.

Ideally, you would like a model to have only slightly lower performance on “new” data on which it was not trained. If you can develop
a model exclusively on training set data that has an AUC over [.72] and the training set data also shows an AUC over [.68], you are
doing very well. But don’t over-optimize your model on the test set data either, or it will turn out to be over-fitted on new,
“validation set” data.

Comment:
12 x
36 x
.3 x
.8 r
((((<.5 is not valid - multiply by -1
.5 means no predictive value
> .90 is too good to be true!)))))

## Question 4
Thresholds greater than 2.5 – not utilizing full range
Thresholds less than 2.5 – not utilizing full range

Comment:
20 x
1000 x
3.5 r
((((Thresholds greater than 2.5 may not be utilizing the full range for analysis � 
Thresholds less than -2.5 0 may not be utilizing the full range for analysis)))))))

## Question 5 (Don't include "$")
Any answer > $1250 has no incremental value – is simply wrong
Any answer > $1000 is of minimal value

Comment:
600 r

## Question 6
If you find that your costs per event on the test set are much higher than your costs per event on the training set, 
consider making your model simpler – probably using fewer input variables – as it is probably still over-fitting the training set data.
Problems that are were not obvious at the ROC-curve stage may emerge when minimizing costs.

Comment:
 200 x
1 x
150 x
700.00 r
((((((If you find that your costs per event on the test set are much higher than
your costs per event on the training set, consider making your model simpler �
probably using fewer input variables as it is probably still over-fitting the �
training set data. Problems with over-fitting that are were not obvious at the ROCcurve stage may emerge when minimizing costs.)))))))))

## Question 7 (Don't include "$")
Must be $1250 minus previous answer.

<=$100 - poor
<100 to <= 250 – good
< 250 to <= 450 very good
>450 – excellent

Comment:
100 x
200 r
(((((((((<=$150 savings is a weak model
<$150 to <= $250 savings is an ok model
< $250 to <= $450 savings is a very good model
>$450 savings is an excellent model))))))))

## Question 8 (Don't include "days")
Take the earning from each card and multiply by the number of credit card applicants per day will give you the money earned per day.

More than a week – poor
4-7 days – very good
2-3 days – excellent
1 day – too good to be true!

Comment:
700000 x
3 r
((((((More than a week poor �
4-7 days very good �
2-3 days excellent �
1 day too good to be true!))))))))) 


## Question 9 (Don't include "%")
Defaulters test incidence 0% - poor
Defaulters test incidence 70 - 100% - poor
Defaulters test incidence >0 < 25% - ok
Defaulters test incidence >25% < 70% good

Any model that is reducing uncertainty will have a True Positive Rate...
Comment:
...Equal to the Test Incidence (% of outcomes classified as "default") x
...Less than the Test Incidence (% of outcomes classified as "default") x
...Greater than the Test Incidence (% of outcomes classified as "default")

## Question 10
Given that the base rate of default in the population is 25%, any test that is reducing uncertainty will have a Positive 
Predictive Value (PPV)...

Comment:
...Equal to .25 x
...Less than .25 x
...Greater than .25

## Question 11
Given that the base rate of default in the population is 25%, any test that is reducing uncertainty will have a Negative Predictive 
Value (NPV)...

Comment:
Equal to .75 x
...Less than .75 x
...Greater than .75

## Question 12
Confusion Matrix Metrics. To determine all performance metrics for a binary classification, it is sufficient to have three values
The Condition Incidence (here the default rate of 25%) The probability of True Positives (the True Positive rate multiplied by the
Condition Incidence)

The Test Incidence (also called classification incidence - the sum of the � � � � probability of True Positives and False Positives)
These three values can all be obtained from the AUC Calculator Spreadsheet and and then used as inputs to the Information Gain
Calculator Spreadsheet to determine all other performance metrics.

AUC_Calculator and Review of AUC Curve.xlsx
Information Gain Calculator.xlsx

Question: What is your model s True Positive Rate? � Save this answer as it will be needed again for Part 3 (Quiz 3)

Comment:
1 x
30 x
.30 r
(((((((<= .25 is incorrect))))))))

## Question 13
Question: What is your model s test incidence ? � � � Save this answer as it will be needed again for Part 3 (Quiz 3)

Comment: TP + FP / total number of sample
0 x
1 x
1000 x
200.00 x
Test Incidences cannot be so small that they force a high false negative rate nor
large that they force a high false positive rate. A perfect test will of course
have a Test Incidence equal to the Condition Incidence but most classification �
systems are focused on avoiding false negatives and have a higher Test Incidence
than Condition Incidence.




